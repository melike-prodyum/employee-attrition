"""
Evaluation Utility Functions
Model deÄŸerlendirme ve metrik hesaplama iÃ§in ortak fonksiyonlar
"""

import numpy as np
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve
)


def calculate_metrics(y_true, y_pred, y_proba, prefix=''):
    """
    Model performans metriklerini hesaplar.
    
    Args:
        y_true: GerÃ§ek deÄŸerler
        y_pred: Tahmin edilen deÄŸerler
        y_proba: Tahmin olasÄ±lÄ±klarÄ±
        prefix: Metrik isimleri iÃ§in prefix (Ã¶rn: 'Train ', 'Val ')
    
    Returns:
        dict: Metrikler dictionary
    """
    metrics = {
        f'{prefix}Accuracy': accuracy_score(y_true, y_pred),
        f'{prefix}Precision': precision_score(y_true, y_pred),
        f'{prefix}Recall': recall_score(y_true, y_pred),
        f'{prefix}F1': f1_score(y_true, y_pred),
        f'{prefix}ROC-AUC': roc_auc_score(y_true, y_proba)
    }
    return metrics


def print_metrics(y_true, y_pred, y_proba, label='Set'):
    """
    Metrikleri yazdÄ±rÄ±r.
    
    Args:
        y_true: GerÃ§ek deÄŸerler
        y_pred: Tahmin edilen deÄŸerler
        y_proba: Tahmin olasÄ±lÄ±klarÄ±
        label: Set etiketi (Ã¶rn: 'Train', 'Validation')
    """
    print(f"\nğŸ”¹ {label} Seti:")
    print(f"  â€¢ Accuracy:  {accuracy_score(y_true, y_pred):.4f}")
    print(f"  â€¢ Precision: {precision_score(y_true, y_pred):.4f}")
    print(f"  â€¢ Recall:    {recall_score(y_true, y_pred):.4f}")
    print(f"  â€¢ F1-Score:  {f1_score(y_true, y_pred):.4f}")
    print(f"  â€¢ ROC-AUC:   {roc_auc_score(y_true, y_proba):.4f}")


def print_classification_report(y_true, y_pred, target_names=None):
    """
    DetaylÄ± sÄ±nÄ±flandÄ±rma raporunu yazdÄ±rÄ±r.
    
    Args:
        y_true: GerÃ§ek deÄŸerler
        y_pred: Tahmin edilen deÄŸerler
        target_names: SÄ±nÄ±f isimleri
    """
    if target_names is None:
        target_names = ['Not Leave (0)', 'Leave (1)']
    
    print("\nğŸ“‹ DetaylÄ± SÄ±nÄ±flandÄ±rma Raporu (Validation):")
    print("-" * 70)
    print(classification_report(y_true, y_pred, target_names=target_names))


def print_confusion_matrix(y_true, y_pred):
    """
    Confusion matrix yazdÄ±rÄ±r.
    
    Args:
        y_true: GerÃ§ek deÄŸerler
        y_pred: Tahmin edilen deÄŸerler
    
    Returns:
        numpy.ndarray: Confusion matrix
    """
    cm = confusion_matrix(y_true, y_pred)
    print("\nğŸ”¢ Confusion Matrix (Validation):")
    print(cm)
    print(f"  True Negatives:  {cm[0, 0]}")
    print(f"  False Positives: {cm[0, 1]}")
    print(f"  False Negatives: {cm[1, 0]}")
    print(f"  True Positives:  {cm[1, 1]}")
    return cm


def print_feature_importance(model, feature_names, top_n=10):
    """
    Feature importance yazdÄ±rÄ±r.
    
    Args:
        model: EÄŸitilmiÅŸ model
        feature_names: Feature isimleri
        top_n: GÃ¶sterilecek en Ã¶nemli feature sayÄ±sÄ±
    
    Returns:
        pd.DataFrame: Feature importance DataFrame
    """
    import pandas as pd
    
    feature_importance = pd.DataFrame({
        'Feature': feature_names,
        'Importance': model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    print(f"\nâ­ En Ã–nemli Ã–zellikler (Top {top_n}):")
    print("-" * 70)
    for idx, row in feature_importance.head(top_n).iterrows():
        print(f"  {row['Feature']:30s} : {row['Importance']:.4f}")
    
    return feature_importance
