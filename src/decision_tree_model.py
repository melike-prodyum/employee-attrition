"""
Employee Attrition Prediction - Decision Tree Model
Ã‡alÄ±ÅŸan iÅŸten ayrÄ±lma tahmini iÃ§in Decision Tree modeli
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve
)
import warnings
warnings.filterwarnings('ignore')

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*70)
print("EMPLOYEE ATTRITION PREDICTION - DECISION TREE MODEL")
print("="*70)

# ============================================================================
# 1. VERÄ° YÃœKLEME VE KEÅÄ°F ANALÄ°ZÄ°
# ============================================================================
print("\n[1] Veri YÃ¼kleme ve KeÅŸif Analizi")
print("-"*70)

# Veri setlerini yÃ¼kle
train_df = pd.read_csv('aug_train.csv')
test_df = pd.read_csv('aug_test.csv')
submission = pd.read_csv('sample_submission.csv')

print(f"âœ“ Train veri seti boyutu: {train_df.shape}")
print(f"âœ“ Test veri seti boyutu: {test_df.shape}")
print(f"\nSÃ¼tunlar: {list(train_df.columns)}")

# Target daÄŸÄ±lÄ±mÄ±
print(f"\nğŸ“Š Target DaÄŸÄ±lÄ±mÄ±:")
print(train_df['target'].value_counts())
print(f"Target oranÄ±: {train_df['target'].value_counts(normalize=True)}")

# Eksik deÄŸerler
print(f"\nğŸ“‹ Eksik DeÄŸerler:")
missing = train_df.isnull().sum()
missing_pct = (missing / len(train_df)) * 100
missing_df = pd.DataFrame({
    'Eksik SayÄ±sÄ±': missing,
    'YÃ¼zde': missing_pct
}).sort_values('Eksik SayÄ±sÄ±', ascending=False)
print(missing_df[missing_df['Eksik SayÄ±sÄ±'] > 0])

# ============================================================================
# 2. VERÄ° Ã–N Ä°ÅLEME
# ============================================================================
print("\n[2] Veri Ã–n Ä°ÅŸleme")
print("-"*70)

# enrollee_id'yi ayÄ±r (model iÃ§in kullanÄ±lmayacak)
train_ids = train_df['enrollee_id']
test_ids = test_df['enrollee_id']

# Target deÄŸiÅŸkeni ayÄ±r
y = train_df['target']
X_train = train_df.drop(['enrollee_id', 'target'], axis=1)
X_test = test_df.drop(['enrollee_id'], axis=1)

print(f"âœ“ Feature sayÄ±sÄ±: {X_train.shape[1]}")

# Kategorik ve numerik sÃ¼tunlarÄ± ayÄ±r
categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()

print(f"âœ“ Kategorik sÃ¼tunlar ({len(categorical_cols)}): {categorical_cols}")
print(f"âœ“ Numerik sÃ¼tunlar ({len(numerical_cols)}): {numerical_cols}")

# Eksik deÄŸerleri doldur
print("\nğŸ”§ Eksik deÄŸerleri doldurma:")

# Numerik sÃ¼tunlar iÃ§in median
for col in numerical_cols:
    if X_train[col].isnull().sum() > 0:
        median_val = X_train[col].median()
        X_train[col].fillna(median_val, inplace=True)
        X_test[col].fillna(median_val, inplace=True)
        print(f"  - {col}: median ile dolduruldu")

# Kategorik sÃ¼tunlar iÃ§in mode (en sÄ±k gÃ¶rÃ¼len deÄŸer)
for col in categorical_cols:
    if X_train[col].isnull().sum() > 0:
        mode_val = X_train[col].mode()[0] if not X_train[col].mode().empty else 'Unknown'
        X_train[col].fillna(mode_val, inplace=True)
        X_test[col].fillna(mode_val, inplace=True)
        print(f"  - {col}: mode ile dolduruldu")

# Kategorik deÄŸiÅŸkenleri encode et
print("\nğŸ”§ Kategorik deÄŸiÅŸkenleri encode etme:")
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    # Train ve test'i birleÅŸtirerek tÃ¼m kategorileri Ã¶ÄŸren
    combined = pd.concat([X_train[col], X_test[col]], axis=0)
    le.fit(combined)
    
    X_train[col] = le.transform(X_train[col])
    X_test[col] = le.transform(X_test[col])
    label_encoders[col] = le
    print(f"  - {col}: {len(le.classes_)} kategori")

print(f"\nâœ“ Veri Ã¶n iÅŸleme tamamlandÄ±!")
print(f"âœ“ Train shape: {X_train.shape}")
print(f"âœ“ Test shape: {X_test.shape}")

# ============================================================================
# 3. DECISION TREE MODELÄ° OLUÅTURMA
# ============================================================================
print("\n[3] Decision Tree Modeli OluÅŸturma")
print("-"*70)

# Train-validation split
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train, y, test_size=0.2, random_state=42, stratify=y
)

print(f"âœ“ Train set: {X_train_split.shape[0]} Ã¶rnekleri")
print(f"âœ“ Validation set: {X_val_split.shape[0]} Ã¶rnekleri")

# Decision Tree modeli - Basit ve az dallÄ± (Random Forest ile karÅŸÄ±laÅŸtÄ±rma iÃ§in)
print("\nğŸŒ³ Decision Tree parametreleri:")
print("  - max_depth: 4 (aÄŸacÄ±n maksimum derinliÄŸi - basit tutuldu)")
print("  - min_samples_split: 200 (dallanma iÃ§in minimum Ã¶rnek sayÄ±sÄ±)")
print("  - min_samples_leaf: 100 (yaprak dÃ¼ÄŸÃ¼mdeki minimum Ã¶rnek sayÄ±sÄ±)")
print("  - criterion: gini (bÃ¶lÃ¼nme kriteri)")
print("  - random_state: 42")

dt_model = DecisionTreeClassifier(
    max_depth=4,                    # Basit ve anlaÅŸÄ±lÄ±r aÄŸaÃ§ iÃ§in dÃ¼ÅŸÃ¼k derinlik
    min_samples_split=200,          # Daha az dallanma iÃ§in artÄ±rÄ±ldÄ±
    min_samples_leaf=100,           # Her yaprakta daha fazla Ã¶rnek - daha az dal
    criterion='gini',               # Gini impurity kullan
    random_state=42,
    class_weight='balanced'         # Dengesiz veri iÃ§in sÄ±nÄ±f aÄŸÄ±rlÄ±klarÄ±
)

print("\nâ³ Model eÄŸitiliyor...")
dt_model.fit(X_train_split, y_train_split)
print("âœ“ Model eÄŸitimi tamamlandÄ±!")

# Model bilgileri
print(f"\nğŸ“Š Model Ã–zellikleri:")
print(f"  - AÄŸaÃ§ derinliÄŸi: {dt_model.get_depth()}")
print(f"  - Yaprak sayÄ±sÄ±: {dt_model.get_n_leaves()}")
print(f"  - Toplam dÃ¼ÄŸÃ¼m sayÄ±sÄ±: {dt_model.tree_.node_count}")

# ============================================================================
# 4. MODEL DEÄERLENDÄ°RME
# ============================================================================
print("\n[4] Model DeÄŸerlendirme")
print("-"*70)

# Tahminler
y_train_pred = dt_model.predict(X_train_split)
y_val_pred = dt_model.predict(X_val_split)
y_train_proba = dt_model.predict_proba(X_train_split)[:, 1]
y_val_proba = dt_model.predict_proba(X_val_split)[:, 1]

# Metrikler
print("\nğŸ“ˆ PERFORMANS METRÄ°KLERÄ°")
print("="*70)

print("\nğŸ”¹ Train Seti:")
print(f"  â€¢ Accuracy:  {accuracy_score(y_train_split, y_train_pred):.4f}")
print(f"  â€¢ Precision: {precision_score(y_train_split, y_train_pred):.4f}")
print(f"  â€¢ Recall:    {recall_score(y_train_split, y_train_pred):.4f}")
print(f"  â€¢ F1-Score:  {f1_score(y_train_split, y_train_pred):.4f}")
print(f"  â€¢ ROC-AUC:   {roc_auc_score(y_train_split, y_train_proba):.4f}")

print("\nğŸ”¹ Validation Seti:")
print(f"  â€¢ Accuracy:  {accuracy_score(y_val_split, y_val_pred):.4f}")
print(f"  â€¢ Precision: {precision_score(y_val_split, y_val_pred):.4f}")
print(f"  â€¢ Recall:    {recall_score(y_val_split, y_val_pred):.4f}")
print(f"  â€¢ F1-Score:  {f1_score(y_val_split, y_val_pred):.4f}")
print(f"  â€¢ ROC-AUC:   {roc_auc_score(y_val_split, y_val_proba):.4f}")

# Classification Report
print("\nğŸ“‹ DetaylÄ± SÄ±nÄ±flandÄ±rma Raporu (Validation):")
print("-"*70)
print(classification_report(y_val_split, y_val_pred, 
                          target_names=['Not Leave (0)', 'Leave (1)']))

# Confusion Matrix
cm = confusion_matrix(y_val_split, y_val_pred)
print("\nğŸ”¢ Confusion Matrix (Validation):")
print(cm)
print(f"  True Negatives:  {cm[0, 0]}")
print(f"  False Positives: {cm[0, 1]}")
print(f"  False Negatives: {cm[1, 0]}")
print(f"  True Positives:  {cm[1, 1]}")

# Feature Importance
print("\nâ­ En Ã–nemli Ã–zellikler (Top 10):")
print("-"*70)
feature_importance = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': dt_model.feature_importances_
}).sort_values('Importance', ascending=False)

for idx, row in feature_importance.head(10).iterrows():
    print(f"  {row['Feature']:30s} : {row['Importance']:.4f}")

# ============================================================================
# 5. GÃ–RSELLEÅTÄ°RME
# ============================================================================
print("\n[5] GÃ¶rselleÅŸtirmeler OluÅŸturuluyor...")
print("-"*70)

# Figure oluÅŸtur
fig = plt.figure(figsize=(20, 12))

# 1. Confusion Matrix
ax1 = plt.subplot(2, 3, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Not Leave', 'Leave'],
            yticklabels=['Not Leave', 'Leave'])
plt.title('Confusion Matrix (Validation)', fontsize=14, fontweight='bold')
plt.ylabel('GerÃ§ek DeÄŸer')
plt.xlabel('Tahmin')

# 2. Feature Importance
ax2 = plt.subplot(2, 3, 2)
top_features = feature_importance.head(10)
plt.barh(range(len(top_features)), top_features['Importance'])
plt.yticks(range(len(top_features)), top_features['Feature'])
plt.xlabel('Importance')
plt.title('Top 10 Ã–nemli Ã–zellikler', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

# 3. ROC Curve
ax3 = plt.subplot(2, 3, 3)
fpr, tpr, _ = roc_curve(y_val_split, y_val_proba)
plt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc_score(y_val_split, y_val_proba):.4f})')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# 4. Target Distribution
ax4 = plt.subplot(2, 3, 4)
target_counts = y.value_counts()
plt.bar(['Not Leave (0)', 'Leave (1)'], target_counts.values, color=['#3498db', '#e74c3c'])
plt.title('Target DaÄŸÄ±lÄ±mÄ± (Train)', fontsize=14, fontweight='bold')
plt.ylabel('SayÄ±')
for i, v in enumerate(target_counts.values):
    plt.text(i, v + 50, str(v), ha='center', fontweight='bold')

# 5. Accuracy Comparison
ax5 = plt.subplot(2, 3, 5)
metrics_train = [
    accuracy_score(y_train_split, y_train_pred),
    precision_score(y_train_split, y_train_pred),
    recall_score(y_train_split, y_train_pred),
    f1_score(y_train_split, y_train_pred)
]
metrics_val = [
    accuracy_score(y_val_split, y_val_pred),
    precision_score(y_val_split, y_val_pred),
    recall_score(y_val_split, y_val_pred),
    f1_score(y_val_split, y_val_pred)
]
x = np.arange(4)
width = 0.35
plt.bar(x - width/2, metrics_train, width, label='Train', color='#2ecc71')
plt.bar(x + width/2, metrics_val, width, label='Validation', color='#3498db')
plt.xlabel('Metrikler')
plt.ylabel('Skor')
plt.title('Model PerformansÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=14, fontweight='bold')
plt.xticks(x, ['Accuracy', 'Precision', 'Recall', 'F1-Score'], rotation=45)
plt.legend()
plt.ylim([0, 1])
plt.grid(True, alpha=0.3, axis='y')

# 6. Decision Tree yapÄ±sÄ±nÄ± gÃ¶ster (basitleÅŸtirilmiÅŸ)
ax6 = plt.subplot(2, 3, 6)
plot_tree(dt_model, 
          max_depth=2,  # GÃ¶rselleÅŸtirme iÃ§in sadece ilk 2 seviye
          filled=True, 
          feature_names=X_train.columns,
          class_names=['Not Leave', 'Leave'],
          fontsize=8,
          rounded=True)
plt.title('Decision Tree YapÄ±sÄ± (Ä°lk 2 Seviye)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig('decision_tree_analysis.png', dpi=300, bbox_inches='tight')
print("âœ“ Grafik kaydedildi: decision_tree_analysis.png")

# Daha detaylÄ± aÄŸaÃ§ gÃ¶rselleÅŸtirmesi
fig2 = plt.figure(figsize=(25, 15))
plot_tree(dt_model, 
          filled=True, 
          feature_names=X_train.columns,
          class_names=['Not Leave', 'Leave'],
          fontsize=10,
          rounded=True,
          proportion=True)
plt.title('Decision Tree - Tam YapÄ±', fontsize=16, fontweight='bold', pad=20)
plt.savefig('decision_tree_full.png', dpi=300, bbox_inches='tight')
print("âœ“ Tam aÄŸaÃ§ gÃ¶rselleÅŸtirmesi kaydedildi: decision_tree_full.png")

# ============================================================================
# 6. TEST VERÄ°SÄ° Ä°Ã‡Ä°N TAHMÄ°NLER
# ============================================================================
print("\n[6] Test Verisi Tahminleri")
print("-"*70)

# TÃ¼m train verisi ile son modeli eÄŸit
print("â³ Final model tÃ¼m train verisi ile eÄŸitiliyor...")
final_model = DecisionTreeClassifier(
    max_depth=4,
    min_samples_split=200,
    min_samples_leaf=100,
    criterion='gini',
    random_state=42,
    class_weight='balanced'
)
final_model.fit(X_train, y)
print("âœ“ Final model eÄŸitimi tamamlandÄ±!")

# Test tahminleri
test_predictions = final_model.predict_proba(X_test)[:, 1]

# Submission dosyasÄ±nÄ± hazÄ±rla
submission['target'] = test_predictions
submission.to_csv('submission_decision_tree.csv', index=False)
print(f"âœ“ Submission dosyasÄ± oluÅŸturuldu: submission_decision_tree.csv")
print(f"âœ“ Tahmin edilen test Ã¶rnekleri: {len(test_predictions)}")
print(f"\nTahmin Ä°statistikleri:")
print(f"  â€¢ Ortalama: {test_predictions.mean():.4f}")
print(f"  â€¢ Std:      {test_predictions.std():.4f}")
print(f"  â€¢ Min:      {test_predictions.min():.4f}")
print(f"  â€¢ Max:      {test_predictions.max():.4f}")

# ============================================================================
# Ã–ZET
# ============================================================================
print("\n" + "="*70)
print("MODEL Ã–ZETI")
print("="*70)
print(f"\nâœ“ Model Tipi: Decision Tree Classifier")
print(f"âœ“ AÄŸaÃ§ DerinliÄŸi: {final_model.get_depth()}")
print(f"âœ“ Yaprak SayÄ±sÄ±: {final_model.get_n_leaves()}")
print(f"âœ“ Validation Accuracy: {accuracy_score(y_val_split, y_val_pred):.4f}")
print(f"âœ“ Validation ROC-AUC: {roc_auc_score(y_val_split, y_val_proba):.4f}")
print(f"\nğŸ“ OluÅŸturulan Dosyalar:")
print(f"  â€¢ decision_tree_analysis.png - Genel analiz grafikleri")
print(f"  â€¢ decision_tree_full.png - Tam aÄŸaÃ§ yapÄ±sÄ±")
print(f"  â€¢ submission_decision_tree.csv - Test tahminleri")

print("\n" + "="*70)
print("âœ… Ä°ÅLEM TAMAMLANDI!")
print("="*70)
print("\nğŸ’¡ Not: Decision Tree basit ve yorumlanabilir bir modeldir.")
print("   Random Forest ile karÅŸÄ±laÅŸtÄ±rma yapmak iÃ§in birden fazla")
print("   aÄŸacÄ±n ensemble'Ä±nÄ± kullanmanÄ±z gerekecek.")
print("="*70)
