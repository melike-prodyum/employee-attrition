"""
Employee Attrition Prediction - Random Forest Model
Ã‡alÄ±ÅŸan iÅŸten ayrÄ±lma tahmini iÃ§in Random Forest modeli
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_curve,
    roc_auc_score
)
import warnings
import os
warnings.filterwarnings('ignore')

# Workspace paths
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
WORKSPACE_ROOT = os.path.dirname(SCRIPT_DIR)
OUTPUT_DIR = os.path.join(WORKSPACE_ROOT, 'outputs', 'random_forest')

# Ortak utility fonksiyonlarÄ±nÄ± import et
from data_utils import (
    load_data,
    print_data_info,
    prepare_features,
    get_column_types,
    fill_missing_values,
    apply_one_hot_encoding,
    create_output_directory,
    create_submission_file
)
from evaluation_utils import (
    print_metrics,
    print_classification_report,
    print_confusion_matrix,
    print_feature_importance
)

# GÃ¶rselleÅŸtirme ayarlarÄ±
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*70)
print("EMPLOYEE ATTRITION PREDICTION - RANDOM FOREST MODEL")
print("="*70)

# ============================================================================
# 1. VERÄ° YÃœKLEME VE KEÅÄ°F ANALÄ°ZÄ°
# ============================================================================
print("\n[1] Veri YÃ¼kleme ve KeÅŸif Analizi")
print("-"*70)

# Veri setlerini yÃ¼kle
train_df, test_df, submission = load_data()
print_data_info(train_df, test_df)

# ============================================================================
# 2. VERÄ° Ã–N Ä°ÅLEME
# ============================================================================
print("\n[2] Veri Ã–n Ä°ÅŸleme")
print("-"*70)

# Features ve target'Ä± ayÄ±r
X_train, X_test, y, train_ids, test_ids = prepare_features(train_df, test_df)

print(f"âœ“ Feature sayÄ±sÄ±: {X_train.shape[1]}")

# Kategorik ve numerik sÃ¼tunlarÄ± ayÄ±r
categorical_cols, numerical_cols = get_column_types(X_train)

print(f"âœ“ Kategorik sÃ¼tunlar ({len(categorical_cols)}): {categorical_cols}")
print(f"âœ“ Numerik sÃ¼tunlar ({len(numerical_cols)}): {numerical_cols}")

# Eksik deÄŸerleri doldur
X_train, X_test = fill_missing_values(X_train, X_test, categorical_cols, numerical_cols)

# Kategorik deÄŸiÅŸkenleri One-Hot Encoding ile encode et (Decision Tree ile tutarlÄ±lÄ±k iÃ§in)
X_train, X_test = apply_one_hot_encoding(X_train, X_test, categorical_cols)

print(f"\nâœ“ Veri Ã¶n iÅŸleme tamamlandÄ±!")
print(f"âœ“ Train shape: {X_train.shape}")
print(f"âœ“ Test shape: {X_test.shape}")

# ============================================================================
# 3. RANDOM FOREST MODELÄ° OLUÅTURMA
# ============================================================================
print("\n[3] Random Forest Modeli OluÅŸturma")
print("-"*70)

# Train-validation split
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train, y, test_size=0.2, random_state=42, stratify=y
)

print(f"âœ“ Train set: {X_train_split.shape[0]} Ã¶rnekleri")
print(f"âœ“ Validation set: {X_val_split.shape[0]} Ã¶rnekleri")

# Random Forest modeli - model_builders'dan al
from model_builders import build_random_forest, get_random_forest_params

rf_params = get_random_forest_params()
print("\nğŸŒ² Random Forest parametreleri:")
print(f"  - n_estimators: {rf_params['n_estimators']} ({rf_params['n_estimators']} farklÄ± decision tree)")
print(f"  - max_depth: {rf_params['max_depth']} (her aÄŸacÄ±n maksimum derinliÄŸi - basit)")
print(f"  - min_samples_split: {rf_params['min_samples_split']} (dallanma iÃ§in minimum Ã¶rnek)")
print(f"  - min_samples_leaf: {rf_params['min_samples_leaf']} (yaprak dÃ¼ÄŸÃ¼mdeki minimum Ã¶rnek)")
print(f"  - criterion: {rf_params['criterion']} (bÃ¶lÃ¼nme kriteri)")
print(f"  - random_state: {rf_params['random_state']}")
print(f"  - class_weight: {rf_params['class_weight']} (dengesiz veri iÃ§in)")
print(f"  - n_jobs: {rf_params['n_jobs']} (paralel iÅŸleme)")
print(f"  - max_features: {rf_params['max_features']} (her dallanmada rastgele feature seÃ§)")

rf_model = build_random_forest()

print("\nâ³ Random Forest eÄŸitiliyor (100 aÄŸaÃ§)...")
rf_model.fit(X_train_split, y_train_split)
print("âœ“ Model eÄŸitimi tamamlandÄ±!")

# Model bilgileri
print(f"\nğŸ“Š Model Ã–zellikleri:")
print(f"  - AÄŸaÃ§ sayÄ±sÄ±: {rf_model.n_estimators}")
print(f"  - Her aÄŸaÃ§ iÃ§in max derinlik: {rf_model.max_depth}")
print(f"  - Toplam estimator: {len(rf_model.estimators_)}")

# Ä°lk birkaÃ§ aÄŸacÄ±n derinliÄŸini gÃ¶ster
tree_depths = [tree.get_depth() for tree in rf_model.estimators_[:5]]
print(f"  - Ä°lk 5 aÄŸacÄ±n derinlikleri: {tree_depths}")

# ============================================================================
# 4. MODEL DEÄERLENDÄ°RME
# ============================================================================
print("\n[4] Model DeÄŸerlendirme")
print("-"*70)

# Tahminler
y_train_pred = rf_model.predict(X_train_split)
y_val_pred = rf_model.predict(X_val_split)
y_train_proba = rf_model.predict_proba(X_train_split)[:, 1]
y_val_proba = rf_model.predict_proba(X_val_split)[:, 1]

# Metrikler
print("\nğŸ“ˆ PERFORMANS METRÄ°KLERÄ°")
print("="*70)

print_metrics(y_train_split, y_train_pred, y_train_proba, 'Train')
print_metrics(y_val_split, y_val_pred, y_val_proba, 'Validation')

# Classification Report
print_classification_report(y_val_split, y_val_pred)

# Confusion Matrix
cm = print_confusion_matrix(y_val_split, y_val_pred)

# Feature Importance
feature_importance = print_feature_importance(rf_model, X_train.columns)

# ============================================================================
# 5. GÃ–RSELLEÅTÄ°RME
# ============================================================================
print("\n[5] GÃ¶rselleÅŸtirmeler OluÅŸturuluyor...")
print("-"*70)

# outputs/random_forest klasÃ¶rÃ¼nÃ¼ oluÅŸtur
output_dir = create_output_directory('random_forest')

# Figure oluÅŸtur - BirleÅŸik gÃ¶rsel
fig = plt.figure(figsize=(20, 14))

# 1. Confusion Matrix
ax1 = plt.subplot(3, 3, 1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False,
            xticklabels=['Not Leave', 'Leave'],
            yticklabels=['Not Leave', 'Leave'])
plt.title('Confusion Matrix (Validation)', fontsize=14, fontweight='bold')
plt.ylabel('GerÃ§ek DeÄŸer')
plt.xlabel('Tahmin')

# 2. Feature Importance
ax2 = plt.subplot(3, 3, 2)
top_features = feature_importance.head(10)
bars = plt.barh(range(len(top_features)), top_features['Importance'], color='#27ae60', alpha=0.7)
plt.yticks(range(len(top_features)), top_features['Feature'])
plt.xlabel('Importance')
plt.title('Top 10 Ã–nemli Ã–zellikler', fontsize=14, fontweight='bold')
# DeÄŸerleri Ã§ubuklarÄ±n Ã¼zerine ekle
for i, (idx, row) in enumerate(top_features.iterrows()):
    plt.text(row['Importance'], i, f' {row["Importance"]:.4f}', 
             va='center', fontsize=9, fontweight='bold')
plt.gca().invert_yaxis()

# 3. ROC Curve
ax3 = plt.subplot(3, 3, 3)
fpr, tpr, _ = roc_curve(y_val_split, y_val_proba)
plt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc_score(y_val_split, y_val_proba):.4f})', color='#27ae60')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# 4. Target Distribution
ax4 = plt.subplot(3, 3, 4)
target_counts = y.value_counts()
plt.bar(['Not Leave (0)', 'Leave (1)'], target_counts.values, color=['#3498db', '#e74c3c'])
plt.title('Target DaÄŸÄ±lÄ±mÄ± (Train)', fontsize=14, fontweight='bold')
plt.ylabel('SayÄ±')
for i, v in enumerate(target_counts.values):
    plt.text(i, v + 50, str(v), ha='center', fontweight='bold')

# 5. Accuracy Comparison
ax5 = plt.subplot(3, 3, 5)
metrics_train = [
    accuracy_score(y_train_split, y_train_pred),
    precision_score(y_train_split, y_train_pred),
    recall_score(y_train_split, y_train_pred),
    f1_score(y_train_split, y_train_pred)
]
metrics_val = [
    accuracy_score(y_val_split, y_val_pred),
    precision_score(y_val_split, y_val_pred),
    recall_score(y_val_split, y_val_pred),
    f1_score(y_val_split, y_val_pred)
]
x = np.arange(4)
width = 0.35
bars1 = plt.bar(x - width/2, metrics_train, width, label='Train', color='#2ecc71', alpha=0.7)
bars2 = plt.bar(x + width/2, metrics_val, width, label='Validation', color='#27ae60', alpha=0.9)
# DeÄŸerleri Ã§ubuklarÄ±n Ã¼zerine ekle
for i, (v1, v2) in enumerate(zip(metrics_train, metrics_val)):
    plt.text(i - width/2, v1 + 0.02, f'{v1:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
    plt.text(i + width/2, v2 + 0.02, f'{v2:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
plt.xlabel('Metrikler')
plt.ylabel('Skor')
plt.title('Model PerformansÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=14, fontweight='bold')
plt.xticks(x, ['Accuracy', 'Precision', 'Recall', 'F1-Score'], rotation=45)
plt.legend()
plt.ylim([0, 1.1])
plt.grid(True, alpha=0.3, axis='y')

# 6-9. Ä°lk 4 aÄŸacÄ±n gÃ¶rselleÅŸtirmesi
print("âœ“ Ä°lk 4 aÄŸacÄ± gÃ¶rselleÅŸtiriyorum...")
for i in range(4):
    ax = plt.subplot(3, 3, 6 + i)
    plot_tree(rf_model.estimators_[i], 
              max_depth=2,  # Sadece ilk 2 seviye gÃ¶ster
              filled=True, 
              feature_names=X_train.columns,
              class_names=['Not Leave', 'Leave'],
              fontsize=7,
              rounded=True)
    plt.title(f'AÄŸaÃ§ #{i+1} (Ä°lk 2 Seviye)', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'random_forest_analysis.png'), dpi=300, bbox_inches='tight')
print("âœ“ BirleÅŸik grafik kaydedildi: outputs/random_forest/random_forest_analysis.png")

# ============================================================================
# AYRI AYRI GRAFÄ°KLER
# ============================================================================
print("\nğŸ“Š Grafikleri ayrÄ± ayrÄ± kaydediyorum...")

# 1. Confusion Matrix - AyrÄ±
fig1 = plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False,
            xticklabels=['Not Leave', 'Leave'],
            yticklabels=['Not Leave', 'Leave'])
plt.title('Confusion Matrix (Validation)', fontsize=14, fontweight='bold')
plt.ylabel('GerÃ§ek DeÄŸer')
plt.xlabel('Tahmin')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'rf_confusion_matrix.png'), dpi=300, bbox_inches='tight')
plt.close()
print("  âœ“ Confusion Matrix kaydedildi")

# 2. Feature Importance - AyrÄ±
fig2 = plt.figure(figsize=(10, 8))
top_features = feature_importance.head(10)
bars = plt.barh(range(len(top_features)), top_features['Importance'], color='#27ae60', alpha=0.7)
plt.yticks(range(len(top_features)), top_features['Feature'])
plt.xlabel('Importance')
plt.title('Top 10 Ã–nemli Ã–zellikler', fontsize=14, fontweight='bold')
# DeÄŸerleri Ã§ubuklarÄ±n Ã¼zerine ekle
for i, (idx, row) in enumerate(top_features.iterrows()):
    plt.text(row['Importance'], i, f' {row["Importance"]:.4f}', 
             va='center', fontsize=10, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'rf_feature_importance.png'), dpi=300, bbox_inches='tight')
plt.close()
print("  âœ“ Feature Importance kaydedildi")

# 3. ROC Curve - AyrÄ±
fig3 = plt.figure(figsize=(8, 6))
fpr, tpr, _ = roc_curve(y_val_split, y_val_proba)
plt.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc_score(y_val_split, y_val_proba):.4f})', color='#27ae60')
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'rf_roc_curve.png'), dpi=300, bbox_inches='tight')
plt.close()
print("  âœ“ ROC Curve kaydedildi")

# 4. Target Distribution - AyrÄ±
fig4 = plt.figure(figsize=(8, 6))
target_counts = y.value_counts()
plt.bar(['Not Leave (0)', 'Leave (1)'], target_counts.values, color=['#3498db', '#e74c3c'])
plt.title('Target DaÄŸÄ±lÄ±mÄ± (Train)', fontsize=14, fontweight='bold')
plt.ylabel('SayÄ±')
for i, v in enumerate(target_counts.values):
    plt.text(i, v + 50, str(v), ha='center', fontweight='bold')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'rf_target_distribution.png'), dpi=300, bbox_inches='tight')
plt.close()
print("  âœ“ Target Distribution kaydedildi")

# 5. Performance Metrics - AyrÄ±
fig5 = plt.figure(figsize=(10, 6))
metrics_train = [
    accuracy_score(y_train_split, y_train_pred),
    precision_score(y_train_split, y_train_pred),
    recall_score(y_train_split, y_train_pred),
    f1_score(y_train_split, y_train_pred)
]
metrics_val = [
    accuracy_score(y_val_split, y_val_pred),
    precision_score(y_val_split, y_val_pred),
    recall_score(y_val_split, y_val_pred),
    f1_score(y_val_split, y_val_pred)
]
x = np.arange(4)
width = 0.35
bars1 = plt.bar(x - width/2, metrics_train, width, label='Train', color='#2ecc71', alpha=0.7)
bars2 = plt.bar(x + width/2, metrics_val, width, label='Validation', color='#27ae60', alpha=0.9)
# DeÄŸerleri Ã§ubuklarÄ±n Ã¼zerine ekle
for i, (v1, v2) in enumerate(zip(metrics_train, metrics_val)):
    plt.text(i - width/2, v1 + 0.02, f'{v1:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
    plt.text(i + width/2, v2 + 0.02, f'{v2:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
plt.xlabel('Metrikler')
plt.ylabel('Skor')
plt.title('Model PerformansÄ± KarÅŸÄ±laÅŸtÄ±rmasÄ±', fontsize=14, fontweight='bold')
plt.xticks(x, ['Accuracy', 'Precision', 'Recall', 'F1-Score'], rotation=45)
plt.legend()
plt.ylim([0, 1.1])
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'rf_performance_metrics.png'), dpi=300, bbox_inches='tight')
plt.close()
print("  âœ“ Performance Metrics kaydedildi")

# 6. Ä°lk 4 aÄŸaÃ§ - AyrÄ± ayrÄ±
for i in range(4):
    fig_tree = plt.figure(figsize=(12, 8))
    plot_tree(rf_model.estimators_[i], 
              max_depth=2,
              filled=True, 
              feature_names=X_train.columns,
              class_names=['Not Leave', 'Leave'],
              fontsize=9,
              rounded=True)
    plt.title(f'Random Forest - AÄŸaÃ§ #{i+1} (Ä°lk 2 Seviye)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, f'rf_tree_{i+1}.png'), dpi=300, bbox_inches='tight')
    plt.close()
print(f"  âœ“ Ä°lk 4 aÄŸaÃ§ ayrÄ± ayrÄ± kaydedildi")

# Tek bir aÄŸacÄ±n tam yapÄ±sÄ±
print("âœ“ Ä°lk aÄŸacÄ±n tam yapÄ±sÄ±nÄ± kaydediyorum...")
fig_full = plt.figure(figsize=(25, 15))
plot_tree(rf_model.estimators_[0], 
          filled=True, 
          feature_names=X_train.columns,
          class_names=['Not Leave', 'Leave'],
          fontsize=10,
          rounded=True,
          proportion=True)
plt.title('Random Forest - Ä°lk AÄŸaÃ§ (Tam YapÄ±)', fontsize=16, fontweight='bold', pad=20)
plt.savefig(os.path.join(OUTPUT_DIR, 'random_forest_single_tree.png'), dpi=300, bbox_inches='tight')
plt.close()
print("  âœ“ Tek aÄŸaÃ§ gÃ¶rselleÅŸtirmesi kaydedildi")

# AÄŸaÃ§ derinlikleri daÄŸÄ±lÄ±mÄ±
fig_stats = plt.figure(figsize=(12, 6))

# Sol: AÄŸaÃ§ derinlikleri
ax1 = plt.subplot(1, 2, 1)
all_depths = [tree.get_depth() for tree in rf_model.estimators_]
plt.hist(all_depths, bins=range(min(all_depths), max(all_depths) + 2), 
         color='#27ae60', alpha=0.7, edgecolor='black')
plt.xlabel('AÄŸaÃ§ DerinliÄŸi', fontweight='bold')
plt.ylabel('AÄŸaÃ§ SayÄ±sÄ±', fontweight='bold')
plt.title('100 AÄŸacÄ±n Derinlik DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')
plt.axvline(np.mean(all_depths), color='red', linestyle='--', 
            label=f'Ortalama: {np.mean(all_depths):.2f}')
plt.legend()
plt.grid(True, alpha=0.3)

# SaÄŸ: Yaprak sayÄ±larÄ±
ax2 = plt.subplot(1, 2, 2)
all_leaves = [tree.get_n_leaves() for tree in rf_model.estimators_]
plt.hist(all_leaves, bins=20, color='#2ecc71', alpha=0.7, edgecolor='black')
plt.xlabel('Yaprak SayÄ±sÄ±', fontweight='bold')
plt.ylabel('AÄŸaÃ§ SayÄ±sÄ±', fontweight='bold')
plt.title('100 AÄŸacÄ±n Yaprak SayÄ±sÄ± DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')
plt.axvline(np.mean(all_leaves), color='red', linestyle='--', 
            label=f'Ortalama: {np.mean(all_leaves):.2f}')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig(os.path.join(OUTPUT_DIR, 'random_forest_tree_stats.png'), dpi=300, bbox_inches='tight')
plt.close()
print("  âœ“ AÄŸaÃ§ istatistikleri kaydedildi")

print("\nâœ“ TÃ¼m grafikler hem birleÅŸik hem de ayrÄ± ayrÄ± kaydedildi!")


# ============================================================================
# 6. TEST VERÄ°SÄ° Ä°Ã‡Ä°N TAHMÄ°NLER
# ============================================================================
print("\n[6] Test Verisi Tahminleri")
print("-"*70)

# TÃ¼m train verisi ile son modeli eÄŸit
print("â³ Final Random Forest modeli tÃ¼m train verisi ile eÄŸitiliyor...")
final_model = build_random_forest()
final_model.fit(X_train, y)
print("âœ“ Final model eÄŸitimi tamamlandÄ±!")

# Submission dosyasÄ±nÄ± hazÄ±rla
test_predictions, submission_df = create_submission_file(
    final_model, X_test, submission, 'submission_random_forest.csv'
)
print(f"âœ“ Submission dosyasÄ± oluÅŸturuldu: submissions/submission_random_forest.csv")
print(f"âœ“ Tahmin edilen test Ã¶rnekleri: {len(test_predictions)}")
print(f"\nTahmin Ä°statistikleri:")
print(f"  â€¢ Ortalama: {test_predictions.mean():.4f}")
print(f"  â€¢ Std:      {test_predictions.std():.4f}")
print(f"  â€¢ Min:      {test_predictions.min():.4f}")
print(f"  â€¢ Max:      {test_predictions.max():.4f}")

# ============================================================================
# Ã–ZET
# ============================================================================
print("\n" + "="*70)
print("MODEL Ã–ZETI")
print("="*70)
print(f"\nâœ“ Model Tipi: Random Forest Classifier")
print(f"âœ“ Toplam AÄŸaÃ§ SayÄ±sÄ±: {final_model.n_estimators}")
print(f"âœ“ Her AÄŸaÃ§ DerinliÄŸi: {final_model.max_depth}")
print(f"âœ“ Ortalama AÄŸaÃ§ DerinliÄŸi: {np.mean([tree.get_depth() for tree in final_model.estimators_]):.2f}")
print(f"âœ“ Ortalama Yaprak SayÄ±sÄ±: {np.mean([tree.get_n_leaves() for tree in final_model.estimators_]):.2f}")
print(f"âœ“ Validation Accuracy: {accuracy_score(y_val_split, y_val_pred):.4f}")
print(f"âœ“ Validation ROC-AUC: {roc_auc_score(y_val_split, y_val_proba):.4f}")
print(f"\nğŸ“ OluÅŸturulan Dosyalar:")
print(f"  BirleÅŸik GÃ¶rsel:")
print(f"    â€¢ outputs/random_forest_analysis.png - Genel analiz grafikleri")
print(f"  AyrÄ± GÃ¶rseller:")
print(f"    â€¢ outputs/rf_confusion_matrix.png")
print(f"    â€¢ outputs/rf_feature_importance.png")
print(f"    â€¢ outputs/rf_roc_curve.png")
print(f"    â€¢ outputs/rf_target_distribution.png")
print(f"    â€¢ outputs/rf_performance_metrics.png")
print(f"    â€¢ outputs/rf_tree_1.png, rf_tree_2.png, rf_tree_3.png, rf_tree_4.png")
print(f"    â€¢ outputs/random_forest_single_tree.png - Tek aÄŸaÃ§ tam yapÄ±sÄ±")
print(f"    â€¢ outputs/random_forest_tree_stats.png - AÄŸaÃ§ istatistikleri")
print(f"  Submission:")
print(f"    â€¢ submissions/submission_random_forest.csv")

print("\n" + "="*70)
print("âœ… Ä°ÅLEM TAMAMLANDI!")
print("="*70)
print("\nğŸ’¡ Random Forest Ã–zellikleri:")
print("   â€¢ 100 farklÄ± decision tree kullanÄ±r (ensemble)")
print("   â€¢ Her aÄŸaÃ§ farklÄ± veri Ã¶rnekleriyle eÄŸitilir (bootstrap)")
print("   â€¢ Her dallanmada rastgele feature seÃ§imi yapar")
print("   â€¢ Final tahmin = TÃ¼m aÄŸaÃ§larÄ±n ortalamasÄ±")
print("   â€¢ Tek aÄŸaÃ§tan daha gÃ¼Ã§lÃ¼ ve robust")
print("="*70)
