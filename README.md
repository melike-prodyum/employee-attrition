# Employee Attrition Prediction
## Ã‡alÄ±ÅŸan Ä°ÅŸten AyrÄ±lma Tahmini - Machine Learning Projesi

Bu proje, Ã§alÄ±ÅŸanlarÄ±n iÅŸten ayrÄ±lma olasÄ±lÄ±ÄŸÄ±nÄ± tahmin etmek iÃ§in **Decision Tree** ve **Random Forest** makine Ã¶ÄŸrenmesi modellerini kullanÄ±r ve bu iki yÃ¶ntem arasÄ±ndaki farklarÄ± gÃ¶sterir.

---

## ğŸ“Š Veri Seti

- **aug_train.csv**: 19,158 eÄŸitim Ã¶rneÄŸi
- **aug_test.csv**: 2,129 test Ã¶rneÄŸi
- **Ã–zellikler**: 12 feature (ÅŸehir, deneyim, eÄŸitim, ÅŸirket bilgileri vb.)
- **Hedef**: Binary sÄ±nÄ±flandÄ±rma (0: Kalmaya devam, 1: Ä°ÅŸten ayrÄ±lacak)

### Veri Ã–zellikleri:
- `city`: Åehir kodu
- `city_development_index`: Åehir geliÅŸmiÅŸlik endeksi
- `gender`: Cinsiyet
- `relevent_experience`: Ä°lgili deneyim durumu
- `enrolled_university`: Ãœniversite kayÄ±t durumu
- `education_level`: EÄŸitim seviyesi
- `major_discipline`: Ana disiplin
- `experience`: Toplam deneyim yÄ±lÄ±
- `company_size`: Åirket bÃ¼yÃ¼klÃ¼ÄŸÃ¼
- `company_type`: Åirket tipi
- `last_new_job`: Son iÅŸ deÄŸiÅŸikliÄŸi
- `training_hours`: EÄŸitim saatleri

---

## ğŸš€ KullanÄ±m

### 1. Decision Tree Modeli
```bash
python src/decision_tree_model.py
```

**Ã‡Ä±ktÄ±lar:**
- `outputs/decision_tree/decision_tree_analysis.png` - BirleÅŸik analiz grafikleri
- `outputs/decision_tree/decision_tree_full.png` - Tam aÄŸaÃ§ yapÄ±sÄ±
- `outputs/decision_tree/dt_confusion_matrix.png` - Confusion matrix
- `outputs/decision_tree/dt_feature_importance.png` - Ã–zellik Ã¶nemleri
- `outputs/decision_tree/dt_roc_curve.png` - ROC eÄŸrisi
- `submissions/submission_decision_tree.csv` - Test tahminleri

### 2. Random Forest Modeli
```bash
python src/random_forest_model.py
```

**Ã‡Ä±ktÄ±lar:**
- `outputs/random_forest/random_forest_analysis.png` - BirleÅŸik analiz grafikleri (4 aÄŸaÃ§ Ã¶rneÄŸi)
- `outputs/random_forest/random_forest_single_tree.png` - Tek aÄŸaÃ§ tam yapÄ±sÄ±
- `outputs/random_forest/random_forest_tree_stats.png` - AÄŸaÃ§ istatistikleri
- `outputs/random_forest/rf_confusion_matrix.png` - Confusion matrix
- `outputs/random_forest/rf_feature_importance.png` - Ã–zellik Ã¶nemleri
- `outputs/random_forest/rf_roc_curve.png` - ROC eÄŸrisi
- `submissions/submission_random_forest.csv` - Test tahminleri

**Not:** Bu model Label Encoding kullanÄ±r

### 3. Model KarÅŸÄ±laÅŸtÄ±rmasÄ± (Decision Tree vs Random Forest)
```bash
python src/compare_models.py
```

**Ã‡Ä±ktÄ±lar:**
- `outputs/compare_models/model_comparison.png` - BirleÅŸik karÅŸÄ±laÅŸtÄ±rma grafikleri
- `outputs/compare_models/compare_metrics.png` - Metrik karÅŸÄ±laÅŸtÄ±rmasÄ±
- `outputs/compare_models/compare_roc_curves.png` - ROC eÄŸrileri karÅŸÄ±laÅŸtÄ±rmasÄ±
- `outputs/compare_models/compare_dt_confusion_matrix.png` - Decision Tree confusion matrix
- `outputs/compare_models/compare_rf_confusion_matrix.png` - Random Forest confusion matrix
- `outputs/compare_models/compare_feature_importance.png` - Ã–zellik Ã¶nemleri karÅŸÄ±laÅŸtÄ±rmasÄ±
- `outputs/compare_models/compare_overfitting.png` - Overfitting analizi

**Not:** Bu karÅŸÄ±laÅŸtÄ±rma One-Hot Encoding kullanÄ±r

---

## ğŸ“ˆ Model PerformanslarÄ±

### Decision Tree (Validation Set)
| Metrik | DeÄŸer |
|--------|-------|
| **Accuracy** | 0.6962 |
| **Precision** | 0.4393 |
| **Recall** | 0.7916 |
| **F1-Score** | 0.5650 |
| **ROC-AUC** | 0.7816 |

**Model Ã–zellikleri:**
- AÄŸaÃ§ DerinliÄŸi: 5
- Yaprak SayÄ±sÄ±: 26
- Tek aÄŸaÃ§ kullanÄ±r
- Basit ve yorumlanabilir

### Random Forest (Validation Set)

#### Label Encoding ile:
| Metrik | DeÄŸer |
|--------|-------|
| **Accuracy** | 0.7677 |
| **Precision** | 0.5272 |
| **Recall** | 0.6586 |
| **F1-Score** | 0.5857 |
| **ROC-AUC** | 0.7877 |

#### One-Hot Encoding ile (Compare Models):
| Metrik | DeÄŸer |
|--------|-------|
| **Accuracy** | 0.7523 |
| **Precision** | 0.5024 |
| **Recall** | 0.6639 |
| **F1-Score** | 0.5719 |
| **ROC-AUC** | 0.7808 |

**Model Ã–zellikleri:**
- AÄŸaÃ§ SayÄ±sÄ±: 100
- Her aÄŸaÃ§ derinliÄŸi: 4
- Ortalama yaprak sayÄ±sÄ±: ~14.21
- Ensemble metodu
- Daha robust ve dengeli
- **En iyi sonuÃ§**: Label Encoding ile

### Ä°yileÅŸmeler (Random Forest vs Decision Tree)

#### Label Encoding ile:
- **Accuracy**: +10.27% â†‘
- **Precision**: +20.01% â†‘
- **Recall**: -16.81% â†“
- **F1-Score**: +3.66% â†‘
- **ROC-AUC**: +0.78% â†‘

#### One-Hot Encoding ile (Compare Models):
- **Accuracy**: +8.06% â†‘
- **Precision**: +14.36% â†‘
- **Recall**: -16.14% â†“
- **F1-Score**: +1.23% â†‘
- **ROC-AUC**: -0.11% â†“

---

## ğŸŒ³ Decision Tree vs ğŸŒ² Random Forest

### Decision Tree
âœ… **Avantajlar:**
- Yorumlanabilir ve anlaÅŸÄ±lÄ±r
- HÄ±zlÄ± eÄŸitim
- Tek model, basit
- GÃ¶rselleÅŸtirilebilir

âŒ **Dezavantajlar:**
- Overfitting riski yÃ¼ksek
- KÃ¼Ã§Ã¼k veri deÄŸiÅŸikliklerinde kararsÄ±z
- DÃ¼ÅŸÃ¼k genelleme

### Random Forest
âœ… **Avantajlar:**
- YÃ¼ksek doÄŸruluk
- Overfitting riski dÃ¼ÅŸÃ¼k
- Robust ve kararlÄ±
- Feature importance gÃ¼venilir

âŒ **Dezavantajlar:**
- YorumlanmasÄ± zor
- YavaÅŸ eÄŸitim
- Daha fazla kaynak gerekir
- Black-box model

---

## ğŸ” Temel Farklar

| Ã–zellik | Decision Tree | Random Forest |
|---------|---------------|---------------|
| AÄŸaÃ§ SayÄ±sÄ± | 1 | 100 |
| AÄŸaÃ§ DerinliÄŸi | 5 | 4 (her biri) |
| Yaprak SayÄ±sÄ± | 26 | ~14.21 (her aÄŸaÃ§) |
| Veri Ã–rnekleme | TÃ¼m veri | Bootstrap sampling |
| Feature SeÃ§imi | TÃ¼m features | Rastgele subset (sqrt) |
| Tahmin | Tek aÄŸaÃ§ | AÄŸaÃ§larÄ±n ortalamasÄ± |
| Yorumlanabilirlik | YÃ¼ksek | DÃ¼ÅŸÃ¼k |
| Accuracy | 69.62% | 76.77% |

---

## ğŸ“Š En Ã–nemli Ã–zellikler

### Decision Tree
1. **city_development_index** (0.6045) - Åehir geliÅŸmiÅŸlik endeksi
2. **company_size_50-99** (0.2238) - Åirket bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (50-99 Ã§alÄ±ÅŸan)
3. **education_level_Graduate** (0.0519) - EÄŸitim seviyesi (Lisans)
4. **relevent_experience** (0.0500) - Ä°lgili deneyim
5. **city_city_103** (0.0209) - Åehir 103

### Random Forest
1. **city_development_index** (0.5407) - Åehir geliÅŸmiÅŸlik endeksi
2. **city** (0.1287) - Åehir kodu
3. **company_size** (0.1045) - Åirket bÃ¼yÃ¼klÃ¼ÄŸÃ¼
4. **enrolled_university** (0.0755) - Ãœniversite kayÄ±t durumu
5. **relevent_experience** (0.0624) - Ä°lgili deneyim

---

## ğŸ› ï¸ Teknolojiler

- **Python 3.13**
- **pandas** - Veri manipÃ¼lasyonu
- **numpy** - SayÄ±sal hesaplamalar
- **scikit-learn** - Machine learning modelleri
- **matplotlib** - GÃ¶rselleÅŸtirme
- **seaborn** - Ä°statistiksel gÃ¶rselleÅŸtirme

---

## ğŸ“ Veri Ã–n Ä°ÅŸleme AdÄ±mlarÄ±

1. **Eksik DeÄŸer Doldurma:**
   - Kategorik deÄŸiÅŸkenler â†’ Mode (en sÄ±k gÃ¶rÃ¼len deÄŸer)

2. **Encoding:**
   - **Decision Tree**: One-Hot Encoding (186 feature)
   - **Random Forest**: Label Encoding (12 feature)
   - **Compare Models**: Her iki model iÃ§in One-Hot Encoding

3. **Train-Validation Split:**
   - 80% Train, 20% Validation
   - Stratified split (sÄ±nÄ±f dengesi korundu)

4. **Class Balancing:**
   - `class_weight='balanced'` parametresi kullanÄ±ldÄ±

---

## ğŸ“‰ Model Hiperparametreleri

### Decision Tree
```python
DecisionTreeClassifier(
    max_depth=5,              # AÄŸaÃ§ derinliÄŸi (5 seviye)
    min_samples_split=100,    # Dallanma iÃ§in min Ã¶rnek
    min_samples_leaf=50,      # Her yaprakta min Ã¶rnek
    criterion='gini',         # BÃ¶lÃ¼nme kriteri
    class_weight='balanced'   # SÄ±nÄ±f dengesi
)
```

### Random Forest
```python
RandomForestClassifier(
    n_estimators=100,         # 100 aÄŸaÃ§
    max_depth=4,              # Her aÄŸaÃ§ iÃ§in max derinlik
    min_samples_split=200,
    min_samples_leaf=100,
    criterion='gini',
    class_weight='balanced',
    n_jobs=-1                 # Paralel iÅŸleme
)
```

---

## ğŸ“‚ Proje YapÄ±sÄ±

```
employee-attrition/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ aug_train.csv                # EÄŸitim verisi
â”‚   â”œâ”€â”€ aug_test.csv                 # Test verisi
â”‚   â””â”€â”€ sample_submission.csv        # Ã–rnek submission formatÄ±
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ decision_tree_model.py       # Decision Tree modeli
â”‚   â”œâ”€â”€ random_forest_model.py       # Random Forest modeli
â”‚   â”œâ”€â”€ compare_models.py            # Model karÅŸÄ±laÅŸtÄ±rmasÄ±
â”‚   â””â”€â”€ model_builders.py            # YardÄ±mcÄ± fonksiyonlar
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ decision_tree/               # DT Ã§Ä±ktÄ±larÄ±
â”‚   â”‚   â”œâ”€â”€ decision_tree_analysis.png
â”‚   â”‚   â”œâ”€â”€ decision_tree_full.png
â”‚   â”‚   â””â”€â”€ ... (diÄŸer grafikler)
â”‚   â”œâ”€â”€ random_forest/               # RF Ã§Ä±ktÄ±larÄ±
â”‚   â”‚   â”œâ”€â”€ random_forest_analysis.png
â”‚   â”‚   â”œâ”€â”€ random_forest_single_tree.png
â”‚   â”‚   â””â”€â”€ ... (diÄŸer grafikler)
â”‚   â””â”€â”€ compare_models/              # KarÅŸÄ±laÅŸtÄ±rma Ã§Ä±ktÄ±larÄ±
â”‚       â”œâ”€â”€ model_comparison.png
â”‚       â””â”€â”€ ... (diÄŸer grafikler)
â”œâ”€â”€ submissions/
â”‚   â”œâ”€â”€ submission_decision_tree.csv # DT tahminleri
â”‚   â””â”€â”€ submission_random_forest.csv # RF tahminleri
â””â”€â”€ README.md                        # Bu dosya
```

---

## ğŸ¯ SonuÃ§lar ve Ã–neriler

### SonuÃ§lar:
1. **Random Forest (Label Encoding)** en yÃ¼ksek accuracy (%76.77) saÄŸladÄ±
2. **Decision Tree** en yÃ¼ksek recall (%79.16) gÃ¶sterdi - daha fazla attrition vakasÄ±nÄ± yakaladÄ±
3. **Random Forest** daha dengeli performans sundu (precision ve recall dengeli)
4. Overfitting, Random Forest'ta daha az gÃ¶rÃ¼ldÃ¼
5. **Decision Tree** tek aÄŸaÃ§la %69.62 accuracy elde etti
6. **Label Encoding** Random Forest iÃ§in One-Hot Encoding'den daha iyi sonuÃ§ verdi
7. **Compare Models** sonuÃ§larÄ±: RF %75.23 vs DT %69.62 (One-Hot Encoding ile)

### Ã–neriler:
- **Ãœretim iÃ§in**: Random Forest (daha gÃ¼venilir)
- **AÃ§Ä±klama gerekiyorsa**: Decision Tree (yorumlanabilir)
- **HÄ±zlÄ± prototip**: Decision Tree (daha hÄ±zlÄ±)
- **En iyi performans**: Random Forest veya Gradient Boosting

---

## ğŸ”® Gelecek GeliÅŸtirmeler

- [ ] Hyperparameter tuning (GridSearchCV)
- [ ] Feature engineering
- [ ] SMOTE ile class balancing
- [ ] Gradient Boosting modelleri (XGBoost, LightGBM)
- [ ] Cross-validation
- [ ] Feature selection
- [ ] Ensemble of ensembles

---

## ğŸ“§ Ä°letiÅŸim

Bu proje, Decision Tree ve Random Forest arasÄ±ndaki farklarÄ± gÃ¶stermek amacÄ±yla oluÅŸturulmuÅŸtur.

**Tarih:** AralÄ±k 2025

---

## ğŸ“œ Lisans

Bu proje eÄŸitim amaÃ§lÄ±dÄ±r.
